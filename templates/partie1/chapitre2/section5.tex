\subsection{L'extraction des regions}

Il existe plusieurs algorithmes pour extraire des \textit{regions} \som{Tu peux dire "régions", ça fera plus propre !} dans AIKON. 
Pour l'extraction des diagrammes astronomiques du projet EIDA, nous retrouvons l'algorithme \textit{Diagram extraction (YOLO model fine-tuned on historical diagrams)}.
Pour VHS, nous avons l'algorithme \textit{Illustration extraction (YOLO model fine-tuned on historical illustrations)}. Ce dernier a été entraîné dans le cadre du projet \gls{enherit} cité précédemment. JE NE COMPRENDS PAS

\som{VHS et EIDA n'utilisent pas de modèles différents à proprement parler, et aucun des deux projets n'utilise un algo d'extraction entraîné par EnHerit. Les deux projets utilisent des versions fine-tunée (réentraînées) de YOLO, qui ont été entraînées sur des données synthétiques, puis sur les données de VHS (ils s'arrêtent là de leur côté), puis sur les données d'EIDA. Ainsi, on évite de démultiplier les entraînements qui nécessitent beaucoup d'annotations.}

\subsection{La reconnaissance et le calcul de similarité}

\subsubsection{La collation des images}

La collation dans le domaine de la philologie désigne l'action de comparer plusieurs témoins d'une même œuvre pour relever leurs différences et leurs similitudes. 

Il existe déjà des outils pour collationner les textes automatiquement depuis longtemps. A la fin des années 1940, Charlton Hinman avait déjà créer une machine nommée le collationneur Hinman ou Hinman Collator en anglais pour collationner des impressions du \textit{Premier Folio} de William Shakespeare. Plus récemment, le logiciel CollateX permet de comparer des versions numériques du texte automatiquement\footcite{kaouaImageCollationMatching2021}. 

Néanmoins, ces outils ne sont pas transposables sur les éléments iconographiques. Les méthodes d'alignement de texte avec transcription et la tokenisation \som{Expliquer en nbp} utilisées sur les textes ne sont pas applicables aux images\footcite{kaouaImageCollationMatching2021}. C'est pour cette raison que les chercheurs du projet AIKON ont développé des algorithmes de reconnaissance et de calcul des similarités.

\som{Vrai en partie, le but plus exact est de retrouver des copies d'un même motif dans un vaste corpus, cf VHS qui s'intéresse à la réutilisation de planches de l'Encyclopédie. Pour moi, la présentation et l'explication des projets doit s'insérer à ce genre de partie pour expliciter le propos.} 


\subsubsection{Les algorithmes présents sur AIKON}

L'algorithme utilisé pour reconnaître automatiquement les similarités et calculer leur score est SegSwap qui a notamment été entraîné sur le \textit{dataset} de Brueghel cité plus haut. \som{SegSwap a été entraîné sur un jeu de données synthétique. Je pense dans tous les cas qu'il n'est pas nécessaire de se focaliser sur les algorithmes et leur entraînement, vu que ce n'est pas le sujet du stage.} Il a également été testé sur deux autres \textit{datasets} d'images représentant des images de scènes urbaines (citer les noms). Il commence par extraire des segments significatifs sur les images. Ensuite, il copie le segment d'une image A sur une image B pour créer une image modifiée synthétique contenant le segment natif et le segment de l'autre image \som{Ce processus est celui utilisé pour générer des données d'entraînement synthétique, du coup.}. Le but est de reconnaître les morceaux d'image copiés entre les deux images et ignorer les zones non modifiées.  Il s'agit d'apprentissage automatique donc il n'a pas besoin d'annotation, il en génère tout seul \footcite{shenLearningCosegmentationSegment2022} vérifier. 

\som{Je pense qu'il n'est pas nécessaire de s'étendre autant du SegSwap, surtout après en avoir parlé plus tôt pour EnHerit. Selon moi, la mention d'EnHerit devrait venir en même temps que l'explication de SegSwap, comme exemple d'utilisation. Cependant, vu le sujet du mémoire, je ne sais pas si c'est bien nécessaire.} 
